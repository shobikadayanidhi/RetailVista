# -*- coding: utf-8 -*-
"""Copy of Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/152T6T-Knn4boi21urv11vcOUSBJfSo_N
"""

import pandas as pd #Installation and Importing
import numpy as np
from faker import Faker
import random
from datetime import datetime, timedelta

fake = Faker() #Step 2: Define Basic Simulation Setup
Faker.seed(42)
random.seed(42)

# Sample Store and Product list
store_ids = [f'ST{str(i).zfill(3)}' for i in range(1, 6)]
product_ids = [f'P{str(i).zfill(3)}' for i in range(1, 11)]

# Create a date range
start_date = datetime(2024, 1, 1)
end_date = datetime(2024, 3, 31)
date_range = pd.date_range(start=start_date, end=end_date, freq='D')  # Daily sales

data = [] #Generating the dataset

for date in date_range:
    for store in store_ids:
        for product in product_ids:
            units_sold = np.random.poisson(lam=20)
            price = round(random.uniform(10, 100), 2)
            revenue = round(units_sold * price, 2)
            promotion = random.choice([0, 1])  # 1 if there was a promotion

            data.append({
                'date': date,
                'store_id': store,
                'product_id': product,
                'units_sold': units_sold,
                'price': price,
                'revenue': revenue,
                'promotion': promotion
            })

df = pd.DataFrame(data)

import os #Saving to CSV for later use

# Create a data folder if not exists
os.makedirs("data", exist_ok=True)
df.to_csv("data/simulated_sales_data.csv", index=False)
print("âœ… Dataset saved as data/simulated_sales_data.csv") #PHASE 1 ENDS

import pandas as pd #real world data PHASE 2 ENDS - EDA STARTS

df = pd.read_csv('supermarket_sales - Sheet1.csv') #Step 1: Load and Inspect Data
df.head()
df.info()
df.describe()
df.isnull().sum()

#df['Date'] = pd.to_datetime(df['Date']) Step2 : Data Cleaning - OPTIONAL
#df.drop_duplicates(inplace=True)

import seaborn as sns #Visual Analysis - Trends and Distirbutions
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go

# Set style
sns.set(style='whitegrid', palette='muted', font_scale=1.1)

# 1. Total Sales by City (Interactive)
city_sales = df.groupby("City")["Sales"].sum().reset_index().sort_values(by="Sales", ascending=False)
fig = px.bar(city_sales, x="City", y="Sales", color="City", text_auto='.2s',
             title="ðŸ’° Total Sales by City", template="plotly_dark")
fig.show()

# 2. Monthly Sales Trend (Interactive Line Plot)
df['Month'] = df['Date'].dt.to_period('M')
monthly_sales = df.groupby("Month")["Sales"].sum().reset_index()
monthly_sales["Month"] = monthly_sales["Month"].astype(str)
fig2 = px.line(monthly_sales, x="Month", y="Sales", markers=True,
               title="ðŸ“ˆ Monthly Sales Trend", template="plotly_white")
fig2.show()

# 3. Distribution of Gross Income by Product Line (Violin Plot)
plt.figure(figsize=(10, 6))
sns.violinplot(data=df, x="Product line", y="gross income", hue="Gender", split=True)
plt.title("ðŸŽ» Distribution of Gross Income by Product Line and Gender")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 4. Heatmap of Average Ratings by Gender & Product Line
pivot_table = df.pivot_table(values="Rating", index="Product line", columns="Gender", aggfunc="mean")
plt.figure(figsize=(8, 6))
sns.heatmap(pivot_table, annot=True, cmap="YlGnBu", fmt=".2f", cbar_kws={'label': 'Avg Rating'})
plt.title("ðŸ”¥ Average Ratings by Product Line & Gender")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# 5. Sunburst Chart â€“ Total Sales Breakdown (City > Branch > Product Line)
fig3 = px.sunburst(df, path=['City', 'Branch', 'Product line'], values='Sales',
                   title='ðŸŒž Sales Breakdown by City > Branch > Product Line')
fig3.show()

# 6. Count Plot of Payment Method
plt.figure(figsize=(7, 5))
sns.countplot(data=df, x="Payment", hue="Gender", palette="Set2")
plt.title("ðŸ’³ Payment Method Distribution by Gender")
plt.tight_layout()
plt.show()

# Correlation heatmap for numerical features
plt.figure(figsize=(10, 6))
correlation = df.corr(numeric_only=True)
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("ðŸ“Š Correlation Heatmap of Numerical Features")
plt.show()

# Boxplots for key numerical columns
numerical_cols = ['Unit price', 'Quantity', 'Sales', 'cogs', 'gross income']
plt.figure(figsize=(14, 8))

for i, col in enumerate(numerical_cols, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(data=df, y=col, color='skyblue')
    plt.title(f"ðŸ“¦ Boxplot of {col}")

plt.tight_layout()
plt.show()

# IQR-based outlier detection function
def detect_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = data[(data[column] < lower) | (data[column] > upper)]
    print(f"{column}: {len(outliers)} outliers")
    return outliers

# Run for selected columns
outlier_summary = {}
for col in numerical_cols:
    outliers = detect_outliers_iqr(df, col)
    outlier_summary[col] = outliers

# Optional: View outliers in one column
outlier_summary['Sales'].head()

# Highlight outliers in boxplots using IQR overlay
numerical_cols = ['Unit price', 'Quantity', 'Sales', 'cogs', 'gross income']
plt.figure(figsize=(14, 8))

for i, col in enumerate(numerical_cols, 1):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    # Define outliers and non-outliers
    outliers = df[(df[col] < lower) | (df[col] > upper)]
    non_outliers = df[(df[col] >= lower) & (df[col] <= upper)]

    # Plot boxplot for all
    plt.subplot(2, 3, i)
    sns.boxplot(y=col, data=non_outliers, color='lightgreen', width=0.3)
    sns.stripplot(y=outliers[col], color='red', size=6, label='Outliers', jitter=0.15)

    plt.title(f"ðŸ“Œ Highlighted Outliers in {col}")

plt.tight_layout()
plt.show()

import pandas as pd #PHASE 3 FEATURUE ENGINEERING
#STEP1 - DATETIME FEATURES
df = pd.read_csv("supermarket_sales - Sheet1.csv")
df['Date'] = pd.to_datetime(df['Date'], format="mixed", dayfirst = False)
df['Day'] = df['Date'].dt.day
df['Month'] = df['Date'].dt.month
df['Weekday'] = df['Date'].dt.day_name()
df['Is_Weekend'] = df['Weekday'].isin(['Saturday', 'Sunday']).astype(int)
df.head()

#STEP2 - SALES FEATURES
df['Total_per_Item'] = df['Sales'] / df['Quantity']
df['Revenue_per_Unit'] = df['gross income'] / df['Quantity']
df['Profit_Margin'] = df['gross income'] / df['Sales']
df.head()

#STEP3 - CUSTOMER LOYALTY FEATURES
df['Repeat_Customer'] = df.duplicated(subset='Invoice ID', keep=False).astype(int)

#STEP4 - PRODUCTIVITY FEATURES
branch_revenue = df.groupby('Branch')['Sales'].transform('sum')
df['Branch_Revenue_Contribution'] = df['Sales'] / branch_revenue

#STEP5 - INTERACTION FEATURES
df['UnitPrice_x_Quantity'] = df['Unit price'] * df['Quantity']
df['Cogs_per_Hour'] = df['cogs'] / pd.to_datetime(df['Time']).dt.hour.replace(0, 1)

#STEP6 - ONE HOT ENCODING
df = pd.get_dummies(df, columns=['Gender', 'Product line', 'Payment'], drop_first=True)

df.head()

from sklearn.model_selection import train_test_split

# Define targets
import pandas as pd
df = pd.read_csv("supermarket_sales - Sheet1.csv")
targets = ['Sales', 'gross income', 'Rating', 'Quantity']
X = df.drop(columns=targets)
y = df[targets]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import seaborn as sns
import matplotlib.pyplot as plt

# Select only numeric columns
numeric_df = df.select_dtypes(include=['number'])

plt.figure(figsize=(16, 12))
sns.heatmap(numeric_df.corr(), annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)
plt.title("Feature Correlation Heatmap", fontsize=18)
plt.show()

import seaborn as sns

sns.pairplot(df[['Unit price', 'Quantity', 'Sales', 'gross income', 'Rating']], diag_kind="kde")
plt.suptitle("Pairwise Relationships Between Key Features", y=1.02)
plt.show()

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
targets = ['Sales', 'gross income', 'Rating', 'Quantity']

for i, ax in enumerate(axes.flat):
    sns.histplot(df[targets[i]], kde=True, ax=ax, color='skyblue')
    ax.set_title(f'Distribution of {targets[i]}')

plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split

# Example target: predicting 'gross income'
# Step 1: Select features and target
X = df.select_dtypes(include=['number']).drop(['gross income', 'Sales', 'Rating'], axis=1)
y_income = df['gross income']
y_sales = df['Sales']
y_rating = df['Rating']

# Step 2: Split the data
X_train_income, X_test_income, y_train_income, y_test_income = train_test_split(X, y_income, test_size=0.2, random_state=42)
X_train_total, X_test_total, y_train_total, y_test_total = train_test_split(X, y_sales, test_size=0.2, random_state=42)
X_train_rating, X_test_rating, y_train_rating, y_test_rating = train_test_split(X, y_rating, test_size=0.2, random_state=42)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Customer Type Encoded'] = le.fit_transform(df['Customer type'])

X_cls = df.select_dtypes(include=['number']).drop(['Customer Type Encoded'], axis=1)
y_cls = df['Customer Type Encoded']

X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)

# Reload original dataset (in case previous preprocessing dropped columns)
import pandas as pd
df = pd.read_csv("supermarket_sales - Sheet1.csv")

# Convert to datetime format
df['Date'] = pd.to_datetime(df['Date'], format="mixed", dayfirst=False)

# Extract day of the week
df['Day_of_Week'] = df['Date'].dt.day_name()

# Group by Date and Product line, and sum gross income
product_profit_df = df.groupby(['Date', 'Product line'])['gross income'].sum().reset_index()

product_profit_df.head()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(16, 8))
sns.lineplot(data=product_profit_df, x="Date", y="gross income", hue="Product line")
plt.title("Profit Trend by Product Line Over Time", fontsize=18)
plt.xlabel("Date")
plt.ylabel("Gross Income")
plt.legend(title="Product Line")
plt.tight_layout()
plt.show()

from prophet import Prophet
# Filter for one product line
health_df = product_profit_df[product_profit_df['Product line'] == 'Health and beauty']

# Prepare columns for Prophet
health_df = health_df.rename(columns={'Date': 'ds', 'gross income': 'y'})

# Instantiate and fit the model
model = Prophet()
model.fit(health_df)

# Future dataframe (next 30 days)
future = model.make_future_dataframe(periods=30)

# Forecast
forecast = model.predict(future)

# Plot forecast
model.plot(forecast)
plt.title("Future Profit Trend: Health and Beauty")
plt.show()

future_profits = {}

for line in df['Product line'].unique():
    temp_df = product_profit_df[product_profit_df['Product line'] == line]
    temp_df = temp_df.rename(columns={'Date': 'ds', 'gross income': 'y'})

    model = Prophet()
    model.fit(temp_df)

    future = model.make_future_dataframe(periods=30)
    forecast = model.predict(future)

    total_future_profit = forecast.tail(30)['yhat'].sum()
    future_profits[line] = total_future_profit

# Display sorted product lines by forecasted profit
sorted_profits = dict(sorted(future_profits.items(), key=lambda item: item[1], reverse=True))

print("ðŸ”® Predicted Future Product Profit Rankings:\n")
for rank, (product, profit) in enumerate(sorted_profits.items(), 1):
    print(f"{rank}. {product}: ${profit:.2f}")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Load dataset
df = pd.read_csv("supermarket_sales - Sheet1.csv")

# Convert 'Total' column to numeric in case it's not
df['Sales'] = pd.to_numeric(df['Sales'], errors='coerce')

# Group by Branch and Product line to calculate total profit
branch_profit = df.groupby(['Branch', 'Product line'])['Sales'].sum().reset_index()

# Pivot the data for heatmap
pivot_profit = branch_profit.pivot(index='Product line', columns='Branch', values='Sales')

# Plot the heatmap
plt.figure(figsize=(12, 7))
sns.heatmap(pivot_profit, annot=True, fmt=".0f", cmap="YlGnBu", linewidths=.5)
plt.title("Profit by Product Line across Branches", fontsize=16)
plt.xlabel("Branch")
plt.ylabel("Product Line")
plt.xticks(rotation=0)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Step 1: Import necessary libraries  Module 3: Store-Level Profitability Analysis
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: Load the dataset
df = pd.read_csv('supermarket_sales - Sheet1.csv')

# Step 3: Ensure 'Total' is numeric
df['Sales'] = pd.to_numeric(df['Sales'], errors='coerce')

# Step 4: Group data to get total profit per branch and product line
branch_profit = df.groupby(['Branch', 'Product line'])['Sales'].sum().reset_index()

# Step 5: Create a pivot table for heatmap
pivot_table = branch_profit.pivot(index='Product line', columns='Branch', values='Sales')

# Step 6: Visualize with heatmap
plt.figure(figsize=(12, 7))
sns.heatmap(pivot_table, annot=True, fmt=".0f", cmap="YlGnBu", linewidths=0.5)
plt.title("ðŸ’° Profit Distribution Across Branches & Product Lines", fontsize=16)
plt.xlabel("Branch")
plt.ylabel("Product Line")
plt.show()

# Step 7: Auto-generate insights
most_profit_branch = pivot_table.sum().idxmax()
most_profit_value = pivot_table.sum().max()

most_profitable_product = pivot_table.loc[:, most_profit_branch].idxmax()
profit_value = pivot_table.loc[most_profitable_product, most_profit_branch]

print(f"ðŸ” Insight: Branch {most_profit_branch} generated the highest overall profit: â‚¹{most_profit_value:,.2f}")
print(f"ðŸ’¡ Top Product Line in Branch {most_profit_branch}: '{most_profitable_product}' with profit â‚¹{profit_value:,.2f}")

# Step 1: Import required libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Step 2: Load the dataset
df = pd.read_csv('supermarket_sales - Sheet1.csv')

# Step 3: Feature Engineering for Clustering
df['Purchase_Hour'] = pd.to_datetime(df['Time']).dt.hour

# Convert categorical to numeric for clustering
df_cluster = df[['Sales', 'Product line', 'Payment', 'Purchase_Hour']].copy()

# Encode Product Line and Payment
df_cluster['Product line'] = df_cluster['Product line'].astype('category').cat.codes
df_cluster['Payment'] = df_cluster['Payment'].astype('category').cat.codes

# Step 4: Feature Scaling
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df_cluster)

# Step 5: Find optimal number of clusters using Elbow Method
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(scaled_features)
    sse.append(kmeans.inertia_)

plt.figure(figsize=(8, 4))
plt.plot(range(1, 11), sse, marker='o')
plt.title("Elbow Method to Determine Optimal Clusters")
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.grid()
plt.show()

# Step 6: Apply KMeans Clustering
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['Customer_Segment'] = kmeans.fit_predict(scaled_features)

# Step 7: Analyze Segment Characteristics
cluster_summary = df.groupby('Customer_Segment')[['Sales', 'Purchase_Hour']].mean()
cluster_summary['Top Product'] = df.groupby('Customer_Segment')['Product line'].agg(lambda x: x.mode()[0])
cluster_summary['Preferred Payment'] = df.groupby('Customer_Segment')['Payment'].agg(lambda x: x.mode()[0])

print("ðŸ“Š Customer Segments Summary:\n")
display(cluster_summary)

plt.figure(figsize=(10, 6)) #Visualizing customer segments
sns.scatterplot(data=df, x='Sales', y='Purchase_Hour', hue='Customer_Segment', palette='Set2')
plt.title("Customer Segments Based on Purchase Behavior")
plt.xlabel("Total Spend")
plt.ylabel("Hour of Purchase")
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Load the dataset
df = pd.read_csv("supermarket_sales - Sheet1.csv")

# Preprocess time
df['Time'] = pd.to_datetime(df['Time']).dt.hour

# Aggregate features for clustering
cluster_df = df.groupby('Invoice ID').agg({
    'Sales': 'sum',
    'Time': 'mean',
    'Product line': lambda x: x.mode()[0],
    'Payment': lambda x: x.mode()[0]
}).reset_index()

# Prepare numerical features for clustering
X = cluster_df[['Sales', 'Time']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')
cluster_df['Segment'] = kmeans.fit_predict(X_scaled)

# Merge cluster labels back to original data
df = df.merge(cluster_df[['Invoice ID', 'Segment']], on='Invoice ID')

# Segment insights generation
segment_insights = []
for segment in sorted(df['Segment'].unique()):
    seg_data = df[df['Segment'] == segment]
    avg_spend = seg_data['Sales'].mean()
    common_hour = int(round(seg_data['Time'].mean()))
    top_product = seg_data['Product line'].mode()[0]
    top_payment = seg_data['Payment'].mode()[0]

    insight = (
        f"Segment {segment}:\n"
        f"- Avg Spend: â‚¹{avg_spend:.2f}\n"
        f"- Shops late evenings (Hour ~{common_hour})\n"
        f"- Top Product: {top_product}\n"
        f"- Payment: {top_payment}\n\n"
        f"ðŸ‘‰ Recommend targeted offers for Segment {segment} via {top_payment} app at {common_hour}:00!"
    )
    segment_insights.append(insight)

# Print the insights
print("\n\n".join(segment_insights))

